# -*- coding: utf-8 -*-
"""Hackathon

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1btn9V5WBh8Xpt1P1SQIePSdeB0rbOz_F

Importing all required libraries and connect to drive
"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from google.colab import drive
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import os
from tensorflow.keras.preprocessing import image

# Note: data is connected to private google drive (code can only be viewed)
# data is sourced from kaggle:
# https://www.kaggle.com/datasets/emirkiv/hairline-fracture-detection-v2
# https://www.kaggle.com/datasets/foyez767/x-ray-images-of-fractured-and-healthy-bones

drive.mount('/content/drive')
data = "/content/drive/My Drive/xraystress"

"""Set image settings and preprocess the data.
We will do a train-validation split to get the model familiar with the data.
For model creation, will use sigmoid activation (non-linear) and adam optimizer.
10 runs.
"""

# Image settings
batch_size = 32
img_height = 256
img_width = 256
seed = 123

# Preprocess
train_ds = image_dataset_from_directory(
    data,
    validation_split=0.2,
    subset="training",
    seed=seed,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

print(train_ds.class_names)
val_ds = image_dataset_from_directory(
    data,
    validation_split=0.2,
    subset="validation",
    seed=seed,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)


model = models.Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

epochs = 10

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

"""Evaluate the test results (and check actual predictions)"""

results = model.evaluate(val_ds, verbose = 0)
print("test loss, test accuracy", results)
# see how it actually performs

test_data = "/content/drive/My Drive/test"
files = [f for f in os.listdir(test_data) if f.endswith((".jpg", ".png"))]

for i in range(len(files)):
  sample = image.load_img(os.path.join(test_data, files[i]), target_size=(256, 256))
  sample = np.array(sample) / 255.0
  prediction = model.predict(np.array([sample]))
  print("Prediction:", 1 if prediction[0] > 0.5 else 0)
  print("Image:", files[i])
  plt.imshow(sample)
  plt.show()